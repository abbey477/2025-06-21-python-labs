# ğŸ—ï¸ Databricks Architecture on AWS

## Understanding Key Databricks Terms and Components

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ›ï¸ LAKEHOUSE PLATFORM (Databricks)                    â”‚
â”‚      Unified platform combining data lake + data warehouse capabilities  â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ğŸ›ï¸ CONTROL PLANE               â”‚  âš¡ COMPUTE/DATA PLANE           â”‚   â”‚
â”‚  â”‚  (Databricks Managed)           â”‚  (Your AWS Account)             â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚                                 â”‚                                 â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚ ğŸ“‹ Workspace            â”‚    â”‚  â”‚ ğŸ’» Databricks Compute   â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                         â”‚    â”‚  â”‚    Cluster              â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Notebooks             â”‚    â”‚  â”‚                         â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Dashboards            â”‚    â”‚  â”‚ EC2 instances running   â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Libraries             â”‚    â”‚  â”‚ your Spark jobs and     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Configurations        â”‚    â”‚  â”‚ ML workloads            â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Web UI interface      â”‚    â”‚  â”‚                         â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ Types:                  â”‚    â”‚   â”‚
â”‚  â”‚                                 â”‚  â”‚ â€¢ All-purpose clusters  â”‚    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚ â€¢ Job clusters          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ ğŸ”§ Cluster Management   â”‚    â”‚  â”‚ â€¢ SQL warehouses        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                         â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Create/configure      â”‚    â”‚                                 â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Autoscaling           â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Termination policies  â”‚    â”‚  â”‚ ğŸš€ Databricks Runtime   â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Monitoring            â”‚    â”‚  â”‚                         â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ Software stack:         â”‚    â”‚   â”‚
â”‚  â”‚                                 â”‚  â”‚ â€¢ Apache Spark          â”‚    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚ â€¢ Delta Lake            â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ âš™ï¸ Jobs Scheduler       â”‚    â”‚  â”‚ â€¢ Python/Scala/R/SQL    â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                         â”‚    â”‚  â”‚ â€¢ ML libraries          â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Workflow orchestrationâ”‚    â”‚  â”‚ â€¢ Photon engine         â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Job runs              â”‚    â”‚  â”‚ â€¢ Optimizations         â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Execution history     â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                                 â”‚   â”‚
â”‚  â”‚                                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚ ğŸ—„ï¸ Databricks Storage   â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ ğŸ” Security & Access    â”‚    â”‚  â”‚                         â”‚    â”‚   â”‚
â”‚  â”‚  â”‚    Control              â”‚    â”‚  â”‚ S3 Buckets containing:  â”‚    â”‚   â”‚
â”‚  â”‚  â”‚                         â”‚    â”‚  â”‚ â€¢ Delta Lake tables     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Authentication        â”‚    â”‚  â”‚ â€¢ Raw data files        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Authorization         â”‚    â”‚  â”‚   (parquet, JSON, CSV)  â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Audit logging         â”‚    â”‚  â”‚ â€¢ ML models & artifacts â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ â€¢ Notebook results      â”‚    â”‚   â”‚
â”‚  â”‚                                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚                                 â”‚                                 â”‚   â”‚
â”‚  â”‚                                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚                                 â”‚  â”‚ ğŸ“Š Metastore            â”‚    â”‚   â”‚
â”‚  â”‚                                 â”‚  â”‚                         â”‚    â”‚   â”‚
â”‚  â”‚                                 â”‚  â”‚ AWS Glue or Hive        â”‚    â”‚   â”‚
â”‚  â”‚                                 â”‚  â”‚ â€¢ Table metadata        â”‚    â”‚   â”‚
â”‚  â”‚                                 â”‚  â”‚ â€¢ Schema definitions    â”‚    â”‚   â”‚
â”‚  â”‚                                 â”‚  â”‚ â€¢ Data locations        â”‚    â”‚   â”‚
â”‚  â”‚                                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  â˜ï¸ CLOUD PROVIDER: Amazon Web Services (AWS)              â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ğŸ–¥ï¸ EC2         â”‚  â”‚ ğŸ’¾ S3          â”‚  â”‚ ğŸ”’ IAM         â”‚  â”‚ ğŸŒ VPC    â”‚ â”‚
â”‚  â”‚                â”‚  â”‚                â”‚  â”‚                â”‚  â”‚           â”‚ â”‚
â”‚  â”‚ Virtual serversâ”‚  â”‚ Object storage â”‚  â”‚ Identity &     â”‚  â”‚ Virtual   â”‚ â”‚
â”‚  â”‚ for compute    â”‚  â”‚ for data lakes â”‚  â”‚ Access         â”‚  â”‚ Network   â”‚ â”‚
â”‚  â”‚ clusters       â”‚  â”‚ and files      â”‚  â”‚ Management     â”‚  â”‚           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Typical Data Processing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User writes    â”‚         â”‚   Submits to     â”‚         â”‚    Launches      â”‚
â”‚   code in        â”‚  â”€â”€â”€â”€>  â”‚   Control Plane  â”‚  â”€â”€â”€â”€>  â”‚    Compute       â”‚
â”‚   Workspace      â”‚         â”‚                  â”‚         â”‚    Cluster       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                    â”‚
                                                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Results back   â”‚         â”‚   Reads/Writes   â”‚         â”‚    Runtime       â”‚
â”‚   to Workspace   â”‚  <â”€â”€â”€â”€  â”‚   S3 Storage     â”‚  <â”€â”€â”€>  â”‚    executes code â”‚
â”‚                  â”‚         â”‚                  â”‚         â”‚    on cluster    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Detailed Term Definitions

### ğŸ›ï¸ Lakehouse Platform
**What it is:** The unified Databricks architecture that combines the flexibility of data lakes with the performance and structure of data warehouses.

**Key characteristics:**
- Single platform for all data, analytics, and AI workloads
- ACID transactions on data lakes via Delta Lake
- Direct querying of raw data files
- Supports batch and streaming data
- Eliminates data silos

---

### ğŸ›ï¸ Control Plane
**What it is:** The Databricks-managed infrastructure that handles orchestration, management, and user interactions.

**Location:** Hosted and managed by Databricks (not in your AWS account)

**Responsibilities:**
- Serving the web UI/workspace
- Managing cluster lifecycle
- Job scheduling and orchestration
- Security and access control
- Monitoring and logging

**Why it matters:** You don't need to maintain this infrastructure; Databricks handles it for you.

---

### âš¡ Compute/Data Plane
**What it is:** The actual compute and storage resources that run in YOUR AWS account.

**Location:** Your AWS VPC (Virtual Private Cloud)

**Contains:**
- EC2 instances running Databricks clusters
- S3 buckets storing your data
- Network configurations
- IAM roles and permissions

**Why it matters:** Your data never leaves your AWS account. You have full control over security, compliance, and costs.

---

### ğŸ“‹ Workspace
**What it is:** Your collaborative development environment accessible through a web browser.

**Contains:**
- **Notebooks:** Interactive documents combining code, visualizations, and markdown
- **Dashboards:** Visual reports and KPIs
- **Libraries:** Custom packages and dependencies
- **Data:** Data browser and table catalog
- **Jobs:** Scheduled workflows
- **Experiments:** ML experiment tracking

**Think of it as:** Your IDE in the cloud, but for data engineering and data science.

---

### ğŸ’» Databricks Compute Cluster
**What it is:** A set of EC2 instances (virtual machines) that execute your data processing workloads.

**Physical reality:** These are actual EC2 servers running in your AWS account's VPC.

**Types:**

1. **All-Purpose Clusters**
   - For interactive development
   - Used with notebooks
   - Can be shared among users
   - Stay running until manually terminated

2. **Job Clusters**
   - For automated production workloads
   - Automatically terminated after job completes
   - Optimized for cost efficiency

3. **SQL Warehouses**
   - Specialized for SQL queries
   - Optimized for BI and analytics

**Configuration includes:**
- Instance types (compute-optimized, memory-optimized, GPU)
- Number of workers (autoscaling range)
- Databricks runtime version
- Libraries and dependencies

---

### ğŸš€ Databricks Runtime
**What it is:** The pre-configured software environment installed on every cluster node.

**Contains:**
- **Apache Spark:** Distributed computing engine
- **Delta Lake:** Storage layer providing ACID transactions
- **Language support:** Python, Scala, R, SQL, Java
- **ML libraries:** scikit-learn, TensorFlow, PyTorch, MLflow
- **Photon:** Databricks' high-performance query engine
- **Optimizations:** Performance enhancements beyond vanilla Spark

**Versions:**
- **Databricks Runtime:** Standard for data engineering
- **Databricks Runtime ML:** Pre-installed ML libraries
- **Databricks Runtime for Genomics:** Specialized for genomic analysis
- **Photon Runtime:** With Photon query engine enabled

**Think of it as:** The operating system and software stack that makes your cluster powerful and easy to use.

---

### ğŸ—„ï¸ Databricks Storage
**What it is:** S3 buckets in your AWS account where all your data physically resides.

**Stores:**

1. **Delta Lake Tables**
   - Optimized Parquet files with transaction logs
   - ACID-compliant table format
   - Time travel and versioning

2. **Raw Data Files**
   - CSV, JSON, Parquet, Avro, ORC
   - Images, videos, documents
   - Log files and streaming data

3. **ML Artifacts**
   - Trained models
   - Feature tables
   - Experiment results

4. **Metadata**
   - Cluster logs
   - Notebook outputs
   - Job results

**Storage architecture:**
- **Workspace storage:** Databricks-managed S3 for workspace artifacts
- **User storage:** Your own S3 buckets (recommended for data)
- **Root storage:** Default storage for clusters and jobs

---

### â˜ï¸ Cloud Provider (AWS)
**What it is:** The underlying infrastructure provider (in this case, Amazon Web Services).

**Databricks leverages these AWS services:**

| AWS Service | Purpose in Databricks |
|-------------|----------------------|
| **EC2** | Compute instances for clusters |
| **S3** | Object storage for data and artifacts |
| **IAM** | Identity and access management, roles for clusters |
| **VPC** | Network isolation and security |
| **Security Groups** | Firewall rules for cluster access |
| **CloudWatch** | Monitoring and logging |
| **KMS** | Encryption key management |
| **Glue Catalog** | Optional metastore for table metadata |
| **STS** | Temporary security credentials |

**Multi-cloud note:** Databricks also supports Azure and Google Cloud Platform with similar architectures.

---

## ğŸ”‘ Key Takeaways

### Architecture Summary

1. **Control Plane (Databricks-managed)**
   - Handles UI, orchestration, and management
   - You don't manage this infrastructure
   - Hosted by Databricks

2. **Data/Compute Plane (Your AWS account)**
   - Runs actual workloads on EC2
   - Stores data in S3
   - You control security and compliance

3. **Separation of Concerns**
   - Management (Control Plane) vs. Execution (Compute Plane)
   - Enables security and compliance
   - Your data never leaves your cloud account

### Data Flow

```
User â†’ Workspace (Control) â†’ Cluster Management (Control) â†’ 
Compute Cluster (Data Plane) â†’ Runtime Execution â†’ S3 Storage (Data Plane) â†’ 
Results â†’ Workspace
```

### Cost Implications

- **Databricks charges:** For platform features (DBUs - Databricks Units)
- **AWS charges:** For EC2 instances, S3 storage, data transfer
- **Total cost:** Databricks DBUs + AWS infrastructure costs

---

## ğŸ¯ Common Use Cases

### 1. Data Engineering
```
Raw data (S3) â†’ Cluster with Runtime â†’ 
Transform with Spark â†’ Clean Delta Tables (S3)
```

### 2. Data Science
```
Data (S3) â†’ Notebook in Workspace â†’ 
ML Runtime Cluster â†’ Train models â†’ 
Store models (S3) + MLflow tracking
```

### 3. BI & Analytics
```
Delta Tables (S3) â†’ SQL Warehouse â†’ 
Dashboards in Workspace â†’ Business insights
```

### 4. Real-time Streaming
```
Kinesis/Kafka â†’ Cluster with Runtime â†’ 
Structured Streaming â†’ Delta Lake (S3) â†’ Real-time dashboards
```

---

## ğŸ”’ Security Model

**Control Plane Security (Databricks manages):**
- User authentication (SSO, SAML)
- Workspace access controls
- API token management

**Data Plane Security (You manage in AWS):**
- VPC configuration and network isolation
- IAM roles and policies
- S3 bucket encryption
- Security groups
- Private connectivity (AWS PrivateLink)

**Data never crosses planes:** Your actual data stays in your AWS account.

---

## ğŸš€ Getting Started Flow

1. **Setup:** Create Databricks workspace linked to your AWS account
2. **Configure:** Set up VPC, IAM roles, S3 buckets
3. **Create:** Launch a cluster with appropriate runtime
4. **Develop:** Write code in notebooks in your workspace
5. **Execute:** Run code on clusters, data processes in data plane
6. **Store:** Results saved to S3 in Delta Lake format
7. **Monitor:** Track performance and costs in workspace and AWS console

---

## ğŸ“– Glossary Quick Reference

| Term | One-Line Definition |
|------|---------------------|
| **Lakehouse Platform** | Unified architecture combining data lake + data warehouse |
| **Control Plane** | Databricks-managed orchestration and UI layer |
| **Data/Compute Plane** | Your AWS account's compute (EC2) and storage (S3) |
| **Workspace** | Web-based collaborative development environment |
| **Compute Cluster** | EC2 instances executing your data workloads |
| **Runtime** | Pre-configured software stack (Spark, Delta, ML libs) on clusters |
| **Storage** | S3 buckets holding your data and Delta tables |
| **Cloud Provider** | AWS infrastructure services (EC2, S3, IAM, VPC) |

---

## ğŸ’¡ Best Practices

### Cluster Management
- Use **job clusters** for production workloads (auto-terminate)
- Use **all-purpose clusters** for development (can be shared)
- Enable **autoscaling** to optimize costs
- Set **auto-termination** timeouts to avoid idle costs

### Storage Organization
- Use **Delta Lake** format for structured data
- Organize data in **medallion architecture** (bronze/silver/gold)
- Implement **partitioning** for large datasets
- Enable **optimization** and **vacuum** commands regularly

### Security
- Use **AWS IAM instance profiles** for cluster access to S3
- Enable **encryption at rest** (S3) and **in transit** (TLS)
- Implement **workspace access controls** by role
- Use **secrets management** for credentials

---

*This document provides a comprehensive overview of Databricks architecture on AWS. For production deployments, consult Databricks and AWS documentation for detailed configuration and security requirements.*
