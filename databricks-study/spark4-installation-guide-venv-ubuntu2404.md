# Apache Spark 4 Installation Checklist & Validation Guide
## Ubuntu 24.04 LTS | PySpark | Jupyter Notebook

**Date:** December 26, 2024  
**Target:** Local laptop setup for learning Spark before Databricks  
**Ubuntu Version:** 24.04 LTS (Noble Numbat)

---

## üìã PRE-INSTALLATION CHECKLIST

### System Requirements
- [ ] Ubuntu 24.04 LTS (Noble Numbat)
- [ ] Minimum 4GB RAM (8GB+ recommended for Spark)
- [ ] Minimum 10GB free disk space (15GB recommended)
- [ ] Internet connection for downloads
- [ ] Terminal/shell access (Ctrl+Alt+T)
- [ ] User account with sudo privileges

### Ubuntu 24.04 Specifics
**What comes pre-installed on Ubuntu 24.04:**
- ‚úÖ Python 3.12.3 (system default)
- ‚úÖ No Python 2 (completely removed in 24.04)
- ‚úÖ Basic build tools (may need updates)
- ‚ö†Ô∏è No Java by default (you have Java 25 installed)

### Current System Status
Check what's already installed:

```bash
# Check Ubuntu version (should show 24.04 LTS Noble Numbat)
lsb_release -a

# Check current Java version (you have Java 25 LTS)
java -version

# Check Python version (should show Python 3.12.x)
python3 --version

# Check if pip is installed
pip3 --version

# Check available disk space (need at least 10GB free)
df -h ~

# Check available memory (need at least 4GB)
free -h

# Check CPU cores (Spark can use all cores)
nproc
```

### Ubuntu 24.04 Important Notes
‚ö†Ô∏è **PEP 668 Compliance:** Ubuntu 24.04 follows PEP 668, which prevents pip from installing packages system-wide to avoid conflicts. We'll use the `--break-system-packages` flag or virtual environments.

---

## üéØ INSTALLATION PLAN

### Component Versions to Install

| Component | Version | Status |
|-----------|---------|--------|
| Java JDK | 17 LTS (alongside Java 25) | ‚¨ú Not Installed |
| Python | 3.12 (system default) | ‚¨ú To Verify |
| Apache Spark | 4.0.1 | ‚¨ú Not Installed |
| PySpark | 4.0.1 | ‚¨ú Not Installed |
| Jupyter Notebook | Latest | ‚¨ú Not Installed |
| pip | Latest | ‚¨ú To Verify |

---

## üì• STEP 1: JAVA INSTALLATION & CONFIGURATION

### Ubuntu 24.04 Java Details
- **Your System:** Java 25 LTS (already installed)
- **Spark Requirement:** Java 17 LTS (need to install)
- **Package Name:** `openjdk-17-jdk` and `openjdk-17-jre`
- **Installation Path:** `/usr/lib/jvm/java-17-openjdk-amd64/`

### 1.1 Install Java 17 LTS (alongside your Java 25)

```bash
# Update package list (important for Ubuntu 24.04)
sudo apt update

# Install OpenJDK 17 JDK and JRE from official Ubuntu 24.04 repositories
sudo apt install openjdk-17-jdk openjdk-17-jre -y

# Wait for installation to complete
```

**Validation:**
```bash
# List all installed Java versions
ls /usr/lib/jvm/

# You should see both:
# - java-17-openjdk-amd64
# - java-25-openjdk-amd64 (your existing installation)

# Check currently active Java version
java -version
```

### 1.2 Configure Java Alternatives (Switch between Java versions)

Ubuntu 24.04 uses the `update-alternatives` system to manage multiple Java versions:

```bash
# Configure default Java runtime (java command)
sudo update-alternatives --config java

# You will see a menu like this:
# Selection    Path                                         Priority
# 0            /usr/lib/jvm/java-25-openjdk-amd64/bin/java   2511
# 1            /usr/lib/jvm/java-17-openjdk-amd64/bin/java   1711
# 2            /usr/lib/jvm/java-25-openjdk-amd64/bin/java   2511

# Type the number for Java 17 (usually option 1) and press Enter

# Also configure Java compiler (javac command)
sudo update-alternatives --config javac

# Again, select Java 17
```

**Alternative Method - Set Java 17 directly:**
```bash
# Set Java 17 as default without interactive prompt
sudo update-alternatives --set java /usr/lib/jvm/java-17-openjdk-amd64/bin/java
sudo update-alternatives --set javac /usr/lib/jvm/java-17-openjdk-amd64/bin/javac
```

### 1.3 Set JAVA_HOME Environment Variable

```bash
# Edit your bash configuration file
nano ~/.bashrc

# Scroll to the end of the file and add these lines:
# ========== Java Configuration for Spark ==========
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH
# ==================================================

# Save and exit:
# Press Ctrl+X, then Y, then Enter

# Reload the configuration immediately
source ~/.bashrc
```

**Alternative - Add to .profile for all shells:**
```bash
# If you use other shells besides bash, add to .profile too
nano ~/.profile

# Add the same JAVA_HOME lines at the end
# Save and exit (Ctrl+X, Y, Enter)

# Reload
source ~/.profile
```

**Validation:**
```bash
# Verify JAVA_HOME is set correctly
echo $JAVA_HOME
# Expected: /usr/lib/jvm/java-17-openjdk-amd64

# Verify Java version is 17 (not 25)
java -version
# Expected output (first line):
# openjdk version "17.0.x" 2024-xx-xx

# Verify javac (compiler) is also Java 17
javac -version
# Expected: javac 17.0.x

# Verify PATH includes Java 17
which java
# Expected: /usr/lib/jvm/java-17-openjdk-amd64/bin/java

# Complete verification
java -version 2>&1 | head -1
```

**Expected Full Output:**
```
openjdk version "17.0.13" 2024-10-15
OpenJDK Runtime Environment (build 17.0.13+11-Ubuntu-2ubuntu124.04)
OpenJDK 64-Bit Server VM (build 17.0.13+11-Ubuntu-2ubuntu124.04, mixed mode, sharing)
```

### 1.4 How to Switch Back to Java 25 (When Needed)

If you need to use Java 25 for other projects:

```bash
# Switch to Java 25
sudo update-alternatives --set java /usr/lib/jvm/java-25-openjdk-amd64/bin/java
sudo update-alternatives --set javac /usr/lib/jvm/java-25-openjdk-amd64/bin/javac

# Update JAVA_HOME (edit ~/.bashrc and change the path)
export JAVA_HOME=/usr/lib/jvm/java-25-openjdk-amd64

# Reload
source ~/.bashrc
```

‚úÖ **Checkpoint:** Java 17 LTS installed and configured as default

---

## üì• STEP 2: PYTHON & PIP SETUP

### Ubuntu 24.04 Python Details
- **Pre-installed:** Python 3.12.3 (system default)
- **Python 2:** Completely removed (not available)
- **Command:** `python3` (no `/usr/bin/python` symlink by default)
- **Package Manager:** pip3 with PEP 668 restrictions
- **Our Approach:** Virtual environments (best practice)

### 2.1 Verify Python 3.12 Installation

```bash
# Check Python version (should show 3.12.x)
python3 --version
# Expected: Python 3.12.3

# Check if python3 is available
which python3
# Expected: /usr/bin/python3

# Verify Python can run
python3 -c "print('Python is working!')"
```

### 2.2 Install pip3 and Virtual Environment Tools

Ubuntu 24.04 may not have pip and venv installed by default:

```bash
# Update package list first
sudo apt update

# Install pip3 from official repositories
sudo apt install python3-pip -y

# Install venv for virtual environments (REQUIRED)
sudo apt install python3-venv -y

# Install python development headers (needed for some packages)
sudo apt install python3-dev -y

# Install build essentials (needed for compiling Python packages)
sudo apt install build-essential -y
```

**Validation:**
```bash
# Check pip3 version
pip3 --version
# Expected: pip 24.x.x from /usr/lib/python3/dist-packages/pip (python 3.12)

# Check venv module
python3 -m venv --help | head -5
# Should show venv usage information

# Check build tools
gcc --version | head -1
```

### 2.3 Understanding PEP 668 in Ubuntu 24.04

‚ö†Ô∏è **Important:** Ubuntu 24.04 implements PEP 668 to prevent pip from modifying system Python packages.

**Why Virtual Environments?**
- ‚úÖ **Isolation:** Each project has its own dependencies
- ‚úÖ **No system conflicts:** Won't break system Python packages
- ‚úÖ **Reproducibility:** Easy to recreate environment
- ‚úÖ **Best Practice:** Industry standard for Python development
- ‚úÖ **Multiple projects:** Different Spark versions for different projects
- ‚úÖ **Clean uninstall:** Just delete the virtual environment folder

### 2.4 Create a Virtual Environment for Spark

We'll create a dedicated virtual environment for our Spark learning:

```bash
# Create a directory for our Spark project
mkdir -p ~/spark-project
cd ~/spark-project

# Create a virtual environment named 'spark-env'
python3 -m venv spark-env

# This creates a spark-env directory with:
# - Python 3.12 interpreter
# - pip (isolated from system)
# - Standard library
# - Empty site-packages for our installations
```

**Understanding the Virtual Environment Structure:**
```bash
# View the created structure
ls -la spark-env/

# You'll see:
# bin/       - Python executable and activation scripts
# include/   - C headers for Python packages
# lib/       - Python packages will go here
# pyvenv.cfg - Configuration file
```

### 2.5 Activate the Virtual Environment

**Every time** you want to work with Spark, activate the environment:

```bash
# Activate the virtual environment
source ~/spark-project/spark-env/bin/activate

# Your prompt will change to show (spark-env):
# (spark-env) user@hostname:~/spark-project$
```

**What Activation Does:**
- Changes `python` and `python3` to use the venv's Python
- Changes `pip` and `pip3` to install packages in the venv
- Adds the venv's `bin/` directory to your PATH
- Isolates you from the system Python

**Validation:**
```bash
# Check which Python is being used (should be in spark-env)
which python3
# Expected: /home/yourusername/spark-project/spark-env/bin/python3

# Check which pip is being used
which pip3
# Expected: /home/yourusername/spark-project/spark-env/bin/pip3

# Check Python version
python3 --version
# Expected: Python 3.12.3

# Inside venv, pip works without restrictions!
pip3 --version
```

### 2.6 Upgrade pip Inside Virtual Environment

```bash
# Make sure virtual environment is activated (you should see (spark-env) in prompt)

# Upgrade pip to latest version (no --break-system-packages needed!)
pip3 install --upgrade pip

# Verify upgrade
pip3 --version
# Expected: pip 24.x.x from /home/yourusername/spark-project/spark-env/lib/python3.12/site-packages/pip
```

### 2.7 Create Activation Shortcut (Optional but Convenient)

Add an alias to easily activate your Spark environment:

```bash
# Edit .bashrc
nano ~/.bashrc

# Add this line at the end:
# ========== Spark Virtual Environment Shortcut ==========
alias spark-activate='source ~/spark-project/spark-env/bin/activate'
# ========================================================

# Save and exit (Ctrl+X, Y, Enter)

# Reload
source ~/.bashrc
```

**Now you can activate with just:**
```bash
spark-activate
```

### 2.8 Virtual Environment Best Practices

**Always activate before working with Spark:**
```bash
# Start a new terminal session
spark-activate

# Now you can use Python and pip freely
python3 script.py
pip3 install some-package
jupyter notebook

# When done, deactivate
deactivate
```

**To check if you're in the virtual environment:**
```bash
# Method 1: Check prompt (should show (spark-env))
# Your prompt shows: (spark-env) user@hostname:~$

# Method 2: Check VIRTUAL_ENV variable
echo $VIRTUAL_ENV
# Expected: /home/yourusername/spark-project/spark-env

# Method 3: Check Python path
which python3
# Expected: /home/yourusername/spark-project/spark-env/bin/python3
```

**Validation:**
```bash
# Activate environment
spark-activate

# Verify you're in the virtual environment
echo $VIRTUAL_ENV
python3 -c "import sys; print(sys.prefix)"
# Both should show path to spark-env

# Test pip works (dry run, won't actually install)
pip3 install --dry-run numpy
# Should work without any errors or warnings

# Deactivate when done testing
deactivate
```

‚úÖ **Checkpoint:** Python 3.12.3 virtual environment created and ready for Spark packages

---

## üí° UNDERSTANDING VIRTUAL ENVIRONMENTS

### What We Just Created:
```
~/spark-project/
‚îú‚îÄ‚îÄ spark-env/              # Virtual environment (don't commit to git)
‚îÇ   ‚îú‚îÄ‚îÄ bin/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ python3        # Isolated Python interpreter
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pip3           # Isolated pip
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ activate       # Activation script
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ lib/
‚îÇ       ‚îî‚îÄ‚îÄ python3.12/
‚îÇ           ‚îî‚îÄ‚îÄ site-packages/  # Packages will install here
‚îî‚îÄ‚îÄ (your notebooks and code will go here)
```

### Virtual Environment Workflow:
1. **Activate:** `source ~/spark-project/spark-env/bin/activate` (or `spark-activate`)
2. **Work:** Install packages, run code, use Jupyter
3. **Deactivate:** `deactivate` (when switching projects or done)

### Common Commands:
```bash
# Activate
spark-activate  # (or: source ~/spark-project/spark-env/bin/activate)

# Install packages (only while activated)
pip3 install package-name

# List installed packages in this environment
pip3 list

# Save requirements for reproducibility
pip3 freeze > requirements.txt

# Recreate environment on another machine
pip3 install -r requirements.txt

# Deactivate
deactivate
```

---

## üì• STEP 3: APACHE SPARK INSTALLATION

### 3.1 Download Apache Spark 4.0.1

```bash
# Create a directory for Spark downloads
mkdir -p ~/spark-downloads
cd ~/spark-downloads

# Download Spark 4.0.1 (pre-built with Hadoop 3)
wget https://dlcdn.apache.org/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz

# Verify download completed
ls -lh spark-4.0.1-bin-hadoop3.tgz
# Should show approximately 400MB file
```

**Alternative if wget is not installed:**
```bash
sudo apt install wget -y
```

### 3.2 Extract and Install Spark

```bash
# Extract the archive
tar -xzf spark-4.0.1-bin-hadoop3.tgz

# Move to a permanent location
sudo mv spark-4.0.1-bin-hadoop3 /opt/spark

# Create a symbolic link for easier version management
sudo ln -s /opt/spark /usr/local/spark

# Set proper permissions for your user
sudo chown -R $USER:$USER /opt/spark

# Verify installation
ls -la /opt/spark/bin/ | head -10
```

### 3.3 Configure Spark Environment Variables

**Important:** These environment variables need to be set globally (not just in virtual environment):

```bash
# Edit .bashrc for global Spark configuration
nano ~/.bashrc

# Scroll to the end and add these lines:
# ========== Apache Spark Configuration ==========
export SPARK_HOME=/opt/spark
export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
export PYSPARK_PYTHON=python3
# Note: We'll set PYSPARK_DRIVER_PYTHON in the virtual environment
# ================================================

# Save and exit (Ctrl+X, then Y, then Enter)

# Reload the configuration
source ~/.bashrc
```

**Why these locations?**
- `SPARK_HOME`: Global location `/opt/spark` (accessible from any environment)
- `PYSPARK_PYTHON`: Points to system Python 3.12 (will use venv Python when activated)
- `PATH`: Makes `spark-shell` and `pyspark` available everywhere

**Validation:**
```bash
# Verify SPARK_HOME is set
echo $SPARK_HOME
# Expected: /opt/spark

# Verify Spark binaries are in PATH
which spark-shell
# Expected: /opt/spark/bin/spark-shell

which pyspark
# Expected: /opt/spark/bin/pyspark

# Test Spark installation (Scala version)
spark-shell --version
# Should show: version 4.0.1

# Quick version check
spark-shell --version 2>&1 | grep "version"
```

**Expected Output:**
```
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 4.0.1
      /_/
                        
Using Scala version 2.13.14
Compiled by user ubuntu on 2024-09-06T06:47:17Z
...
```

‚úÖ **Checkpoint:** Apache Spark 4.0.1 installed globally at /opt/spark

---

## üì• STEP 4: PYSPARK INSTALLATION IN VIRTUAL ENVIRONMENT

### 4.1 Activate Your Spark Virtual Environment

**IMPORTANT:** Before installing any Python packages, activate your virtual environment:

```bash
# Navigate to your project directory
cd ~/spark-project

# Activate the virtual environment (use your shortcut if you created it)
source spark-env/bin/activate

# OR use the alias if you created it:
spark-activate

# Your prompt should now show: (spark-env) user@hostname:~/spark-project$
```

**Verify you're in the virtual environment:**
```bash
# Check that you're using the venv's Python
which python3
# Expected: /home/yourusername/spark-project/spark-env/bin/python3

# Check pip location
which pip3
# Expected: /home/yourusername/spark-project/spark-env/bin/pip3

# Check VIRTUAL_ENV variable
echo $VIRTUAL_ENV
# Expected: /home/yourusername/spark-project/spark-env
```

### 4.2 Install PySpark 4.0.1

Now install PySpark in the virtual environment (no --break-system-packages needed!):

```bash
# Make sure (spark-env) shows in your prompt!

# Install PySpark 4.0.1 (must match Spark version exactly)
pip3 install pyspark==4.0.1

# This will download and install:
# - pyspark 4.0.1
# - py4j (dependency for PySpark)
# Installation takes 2-3 minutes
```

**What happens during installation:**
- Downloads PySpark package (~300MB)
- Installs in `~/spark-project/spark-env/lib/python3.12/site-packages/`
- Does NOT affect system Python packages
- Only available when virtual environment is activated

### 4.3 Install Essential Data Science Libraries

Install additional libraries in the same virtual environment:

```bash
# Make sure you're still in the activated virtual environment!
# You should see (spark-env) in your prompt

# Install NumPy (numerical computing)
pip3 install numpy

# Install Pandas (data manipulation)  
pip3 install pandas

# Install PyArrow (columnar data format, required by Spark for Pandas operations)
pip3 install pyarrow

# Install Matplotlib (plotting - useful for data visualization)
pip3 install matplotlib

# Install Seaborn (statistical visualization - optional)
pip3 install seaborn

# Install all at once (alternative command):
pip3 install numpy pandas pyarrow matplotlib seaborn
```

**Installation notes:**
- These packages compile C extensions (takes 5-10 minutes on first install)
- Build tools we installed earlier are required
- Everything installs in the virtual environment only

### 4.4 Validate PySpark Installation

```bash
# Make sure virtual environment is still activated!

# Check installed packages
pip3 list | grep -E "pyspark|numpy|pandas|pyarrow"

# Get detailed PySpark info
pip3 show pyspark
```

**Expected output:**
```
Name: pyspark
Version: 4.0.1
Summary: Apache Spark Python API
Home-page: https://github.com/apache/spark/tree/master/python
Author: Spark Developers
License: http://www.apache.org/licenses/LICENSE-2.0
Location: /home/yourusername/spark-project/spark-env/lib/python3.12/site-packages
Requires: py4j
```

### 4.5 Test PySpark Import and Version

```bash
# Test basic import
python3 -c "import pyspark; print(f'PySpark Version: {pyspark.__version__}')"
# Expected: PySpark Version: 4.0.1

# Test SparkSession import
python3 -c "from pyspark.sql import SparkSession; print('‚úì SparkSession imports successfully')"

# Test all data science libraries
python3 << 'EOF'
import pyspark
import numpy as np
import pandas as pd
import pyarrow as pa
import matplotlib

print("=" * 50)
print("INSTALLED PACKAGES IN VIRTUAL ENVIRONMENT")
print("=" * 50)
print(f"‚úì PySpark:    {pyspark.__version__}")
print(f"‚úì NumPy:      {np.__version__}")
print(f"‚úì Pandas:     {pd.__version__}")
print(f"‚úì PyArrow:    {pa.__version__}")
print(f"‚úì Matplotlib: {matplotlib.__version__}")
print("=" * 50)
EOF
```

### 4.6 Test PySpark with Spark Installation

Create a comprehensive test to ensure PySpark can connect to Spark:

```bash
# Make sure you're in the virtual environment
# Create and run a test script

python3 << 'EOF'
from pyspark.sql import SparkSession

print("\n" + "=" * 60)
print("TESTING PYSPARK WITH APACHE SPARK")
print("=" * 60)

try:
    # Create Spark Session
    spark = SparkSession.builder \
        .appName("VirtualEnvTest") \
        .master("local[*]") \
        .getOrCreate()
    
    print(f"‚úì Spark Session created successfully")
    print(f"‚úì Spark Version: {spark.version}")
    print(f"‚úì Master: {spark.sparkContext.master}")
    print(f"‚úì App Name: {spark.sparkContext.appName}")
    
    # Create a test DataFrame
    data = [
        ("Alice", 25, "Engineer"),
        ("Bob", 30, "Data Scientist"),
        ("Charlie", 35, "Manager"),
        ("Diana", 28, "Analyst")
    ]
    columns = ["Name", "Age", "Role"]
    
    df = spark.createDataFrame(data, columns)
    print(f"\n‚úì DataFrame created with {df.count()} rows")
    
    # Show the DataFrame
    print("\nSample Data:")
    df.show()
    
    # Perform transformations
    from pyspark.sql.functions import col
    
    senior_employees = df.filter(col("Age") > 28).select("Name", "Role")
    print(f"\n‚úì Filter operation successful")
    print(f"  Employees over 28 years old:")
    senior_employees.show()
    
    # Test aggregation
    avg_age = df.agg({"Age": "avg"}).collect()[0][0]
    print(f"\n‚úì Aggregation successful")
    print(f"  Average Age: {avg_age:.1f}")
    
    # Stop Spark session
    spark.stop()
    print("\n‚úì Spark session stopped cleanly")
    print("=" * 60)
    print("ALL TESTS PASSED! PySpark is working correctly!")
    print("=" * 60 + "\n")
    
except Exception as e:
    print(f"\n‚úó ERROR: {e}")
    import traceback
    traceback.print_exc()
EOF
```

**Expected output:**
```
============================================================
TESTING PYSPARK WITH APACHE SPARK
============================================================
‚úì Spark Session created successfully
‚úì Spark Version: 4.0.1
‚úì Master: local[*]
‚úì App Name: VirtualEnvTest

‚úì DataFrame created with 4 rows

Sample Data:
+-------+---+--------------+
|   Name|Age|          Role|
+-------+---+--------------+
|  Alice| 25|      Engineer|
|    Bob| 30|Data Scientist|
|Charlie| 35|       Manager|
|  Diana| 28|       Analyst|
+-------+---+--------------+

‚úì Filter operation successful
  Employees over 28 years old:
+-------+--------------+
|   Name|          Role|
+-------+--------------+
|    Bob|Data Scientist|
|Charlie|       Manager|
+-------+--------------+

‚úì Aggregation successful
  Average Age: 29.5

‚úì Spark session stopped cleanly
============================================================
ALL TESTS PASSED! PySpark is working correctly!
============================================================
```

### 4.7 Save Your Environment Requirements

Good practice: save your installed packages for reproducibility:

```bash
# Make sure you're in the activated virtual environment

# Save all installed packages to a file
pip3 freeze > ~/spark-project/requirements.txt

# View the file
cat ~/spark-project/requirements.txt
```

**This file allows you to recreate the environment later:**
```bash
# On another machine or after deleting the venv:
python3 -m venv new-spark-env
source new-spark-env/bin/activate
pip3 install -r requirements.txt
```

### 4.8 Understanding the Virtual Environment Setup

**Your project structure now looks like:**
```
~/spark-project/
‚îú‚îÄ‚îÄ spark-env/              # Virtual environment
‚îÇ   ‚îú‚îÄ‚îÄ bin/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ python3        # Python 3.12 (isolated)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pip3           # pip (isolated)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ activate       # Activation script
‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ python3.12/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ site-packages/
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ pyspark/          # PySpark 4.0.1
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ numpy/            # NumPy
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ pandas/           # Pandas
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ pyarrow/          # PyArrow
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ pyvenv.cfg
‚îî‚îÄ‚îÄ requirements.txt        # Package list
```

**Important notes:**
- PySpark is installed ONLY in the virtual environment
- System Python is NOT affected
- Must activate virtual environment to use PySpark
- Spark installation at `/opt/spark` is global (accessible from venv)

‚úÖ **Checkpoint:** PySpark 4.0.1 and data science libraries installed in virtual environment

---

## üì• STEP 5: JUPYTER NOTEBOOK INSTALLATION IN VIRTUAL ENVIRONMENT

### Ubuntu 24.04 Jupyter Notes
- Jupyter works well with Python 3.12
- We'll install Jupyter INSIDE our virtual environment
- This ensures Jupyter uses the same Python and packages as PySpark

### 5.1 Install Jupyter Notebook and JupyterLab

**Make sure your virtual environment is activated first!**

```bash
# Verify you're in the virtual environment
echo $VIRTUAL_ENV
# Should show: /home/yourusername/spark-project/spark-env

# If not activated, activate it:
spark-activate  # (or: source ~/spark-project/spark-env/bin/activate)

# Now install Jupyter (will install in the virtual environment)
pip3 install jupyter notebook

# Install JupyterLab (modern interface - recommended)
pip3 install jupyterlab

# Install IPython (enhanced interactive Python shell)
pip3 install ipython

# Install Jupyter widgets for interactive visualizations
pip3 install ipywidgets

# Install all at once (alternative):
pip3 install jupyter notebook jupyterlab ipython ipywidgets
```

**Installation takes 3-5 minutes. All components install in the virtual environment.**

**Validation:**
```bash
# Make sure venv is activated!

# Verify Jupyter installation
jupyter --version

# Check Jupyter Notebook
jupyter notebook --version

# Check JupyterLab (if installed)
jupyter lab --version

# Verify jupyter is from the venv
which jupyter
# Expected: /home/yourusername/spark-project/spark-env/bin/jupyter

# List installed Jupyter kernels
jupyter kernelspec list
```

**Expected Output:**
```
Selected Jupyter core packages...
IPython          : 8.x.x
ipykernel        : 6.x.x
ipywidgets       : 8.x.x
jupyter_client   : 8.x.x
jupyter_core     : 5.x.x
jupyter_server   : 2.x.x
jupyterlab       : 4.x.x
nbclient         : 0.x.x
nbconvert        : 7.x.x
nbformat         : 5.x.x
notebook         : 7.x.x
```

### 5.2 Create Notebooks Directory

```bash
# Create a directory for your Jupyter notebooks
mkdir -p ~/spark-project/notebooks

# Navigate to it
cd ~/spark-project/notebooks
```

**Your project structure now:**
```
~/spark-project/
‚îú‚îÄ‚îÄ spark-env/          # Virtual environment with all packages
‚îú‚îÄ‚îÄ notebooks/          # Your Jupyter notebooks go here
‚îî‚îÄ‚îÄ requirements.txt    # Package list
```

### 5.3 Start Jupyter Notebook

**Always activate the virtual environment before starting Jupyter:**

```bash
# Make sure you're in the virtual environment
spark-activate

# Navigate to notebooks directory
cd ~/spark-project/notebooks

# Start Jupyter Notebook
jupyter notebook

# OR start JupyterLab (modern interface):
jupyter lab
```

**Jupyter will:**
1. Start a local server (usually on port 8888)
2. Automatically open your default browser
3. Show the notebooks directory

**If browser doesn't open automatically:**
```bash
# Look for a URL in the terminal output like:
# http://localhost:8888/tree?token=abc123...

# Copy and paste this URL into your browser
```

### 5.4 Test PySpark in Jupyter Notebook

**In your browser:**

1. Click "New" ‚Üí "Notebook" ‚Üí "Python 3"
2. In the first cell, type:

```python
# Cell 1: Import and check versions
import pyspark
import sys

print(f"Python: {sys.version}")
print(f"PySpark: {pyspark.__version__}")
print(f"Python executable: {sys.executable}")
```

3. Press `Shift+Enter` to run

**Expected output:**
```
Python: 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]
PySpark: 4.0.1
Python executable: /home/yourusername/spark-project/spark-env/bin/python3
```

4. In the next cell, create a Spark session:

```python
# Cell 2: Create Spark Session
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("JupyterTest") \
    .master("local[*]") \
    .getOrCreate()

print(f"Spark Version: {spark.version}")
print(f"Spark UI: http://localhost:4040")
```

5. Create and display a DataFrame:

```python
# Cell 3: Create sample DataFrame
data = [
    ("Alice", 25, "Engineer"),
    ("Bob", 30, "Data Scientist"),
    ("Charlie", 35, "Manager")
]

df = spark.createDataFrame(data, ["Name", "Age", "Role"])
df.show()
```

**Expected output:**
```
+-------+---+--------------+
|   Name|Age|          Role|
+-------+---+--------------+
|  Alice| 25|      Engineer|
|    Bob| 30|Data Scientist|
|Charlie| 35|       Manager|
+-------+---+--------------+
```

6. While Jupyter is running, open http://localhost:4040 in another browser tab to see the Spark UI

### 5.5 Create a Startup Script for Convenience

Make it easier to start your Spark/Jupyter environment:

```bash
# Create a startup script
cat > ~/spark-project/start-jupyter.sh << 'EOF'
#!/bin/bash

echo "=================================="
echo "Starting Spark Jupyter Environment"
echo "=================================="

# Activate virtual environment
source ~/spark-project/spark-env/bin/activate

# Navigate to notebooks directory
cd ~/spark-project/notebooks

# Start JupyterLab (or change to 'jupyter notebook' if you prefer)
echo "Starting JupyterLab..."
echo "Press Ctrl+C to stop"
echo "=================================="

jupyter lab
EOF

# Make it executable
chmod +x ~/spark-project/start-jupyter.sh
```

**Now you can start Jupyter with:**
```bash
~/spark-project/start-jupyter.sh
```

### 5.6 Stopping Jupyter

**To stop Jupyter:**
1. Go to the terminal where Jupyter is running
2. Press `Ctrl+C`
3. Confirm with `y` when asked
4. Optionally deactivate the virtual environment: `deactivate`

### 5.7 Virtual Environment Workflow Summary

**Daily workflow:**

```bash
# 1. Activate environment
spark-activate

# 2. Start Jupyter
cd ~/spark-project/notebooks
jupyter lab

# 3. Work on your notebooks...

# 4. Stop Jupyter (Ctrl+C in terminal)

# 5. Deactivate environment (optional)
deactivate
```

**Or use the startup script:**
```bash
~/spark-project/start-jupyter.sh
```

### 5.8 Ubuntu 24.04 Browser Considerations

If Jupyter doesn't open in your browser automatically:

```bash
# Start Jupyter without auto-opening browser
jupyter lab --no-browser

# Copy the URL shown (looks like http://localhost:8888/lab?token=...)
# Paste it into your browser (Firefox, Chrome, etc.)

# Or specify a browser:
BROWSER=firefox jupyter lab
BROWSER=google-chrome jupyter lab
```

### 5.9 Create a Sample Notebook

Create a sample notebook to verify everything works:

```bash
# Make sure venv is activated and you're in notebooks directory
cd ~/spark-project/notebooks

# Create a sample notebook file
cat > spark_test_notebook.ipynb << 'EOF'
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Test Notebook\n",
    "Testing PySpark 4.0.1 with Jupyter on Ubuntu 24.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestNotebook\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úì Spark {spark.version} is running!\")\n",
    "print(f\"‚úì Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create sample data\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF

# Start Jupyter and open this notebook
jupyter lab spark_test_notebook.ipynb
```

‚úÖ **Checkpoint:** Jupyter Notebook/Lab installed in virtual environment and working with PySpark

---

## ‚úÖ STEP 6: COMPREHENSIVE VALIDATION TESTS

### 6.1 System Environment Validation

Create and run this validation script:

```bash
# Create validation script
cat > ~/validate-spark-setup.sh << 'EOF'
#!/bin/bash

echo "=========================================="
echo "SPARK 4.0.1 INSTALLATION VALIDATION"
echo "=========================================="
echo ""

echo "1. Java Version:"
java -version 2>&1 | head -1
echo ""

echo "2. Python Version:"
python3 --version
echo ""

echo "3. Pip Version:"
pip3 --version
echo ""

echo "4. Environment Variables:"
echo "   JAVA_HOME: $JAVA_HOME"
echo "   SPARK_HOME: $SPARK_HOME"
echo "   PYSPARK_PYTHON: $PYSPARK_PYTHON"
echo ""

echo "5. Spark Installation:"
ls -ld /opt/spark 2>/dev/null && echo "   ‚úì Spark directory exists" || echo "   ‚úó Spark directory missing"
echo ""

echo "6. PySpark Version:"
python3 -c "import pyspark; print('   ' + pyspark.__version__)"
echo ""

echo "7. Jupyter Installation:"
jupyter --version 2>&1 | head -1
echo ""

echo "8. Required Python Packages:"
for pkg in pyspark numpy pandas pyarrow; do
    python3 -c "import $pkg; print('   ‚úì $pkg installed')" 2>/dev/null || echo "   ‚úó $pkg missing"
done
echo ""

echo "=========================================="
echo "VALIDATION COMPLETE"
echo "=========================================="
EOF

# Make it executable
chmod +x ~/validate-spark-setup.sh

# Run the validation
~/validate-spark-setup.sh
```

### 6.2 PySpark Functionality Test

Create a simple Python test:

```bash
# Create a test script
cat > ~/test-pyspark.py << 'EOF'
#!/usr/bin/env python3

print("Testing PySpark Installation...")
print("=" * 50)

try:
    from pyspark.sql import SparkSession
    
    # Create Spark session
    spark = SparkSession.builder \
        .appName("InstallationTest") \
        .master("local[*]") \
        .getOrCreate()
    
    print("‚úì Spark Session created successfully")
    print(f"‚úì Spark Version: {spark.version}")
    print(f"‚úì Master URL: {spark.sparkContext.master}")
    
    # Create a simple DataFrame
    data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
    columns = ["Name", "Age"]
    df = spark.createDataFrame(data, columns)
    
    print("‚úì DataFrame created successfully")
    print("\nSample Data:")
    df.show()
    
    # Perform a simple operation
    result = df.filter(df.Age > 25).count()
    print(f"‚úì Filter operation successful: {result} rows with Age > 25")
    
    # Stop the session
    spark.stop()
    print("\n‚úì All tests passed!")
    print("=" * 50)
    
except Exception as e:
    print(f"‚úó Error: {e}")
    import traceback
    traceback.print_exc()
EOF

# Make it executable
chmod +x ~/test-pyspark.py

# Run the test
python3 ~/test-pyspark.py
```

**Expected Output:**
```
Testing PySpark Installation...
==================================================
‚úì Spark Session created successfully
‚úì Spark Version: 4.0.1
‚úì Master URL: local[*]
‚úì DataFrame created successfully

Sample Data:
+-------+---+
|   Name|Age|
+-------+---+
|  Alice| 25|
|    Bob| 30|
|Charlie| 35|
+-------+---+

‚úì Filter operation successful: 2 rows with Age > 25

‚úì All tests passed!
==================================================
```

### 6.3 Jupyter Notebook Test

```bash
# Create a test notebook directory
mkdir -p ~/spark-notebooks
cd ~/spark-notebooks

# Start Jupyter Notebook (will open in browser)
jupyter notebook
```

In the Jupyter Notebook interface:
1. Create a new Python 3 notebook
2. Run this code in the first cell:

```python
from pyspark.sql import SparkSession

# Create Spark Session
spark = SparkSession.builder \
    .appName("JupyterTest") \
    .master("local[*]") \
    .getOrCreate()

print(f"Spark Version: {spark.version}")

# Create test DataFrame
data = [(1, "A"), (2, "B"), (3, "C")]
df = spark.createDataFrame(data, ["id", "letter"])
df.show()

# Check Spark UI
print(f"Spark UI available at: http://localhost:4040")
```

3. Expected output: DataFrame displays correctly in Jupyter

‚úÖ **Checkpoint:** All components validated and working

---

## üéØ QUICK REFERENCE COMMANDS

### Start PySpark Shell
```bash
# Basic PySpark shell
pyspark

# PySpark with custom configuration
pyspark --master local[4] --driver-memory 4g
```

### Start Jupyter Notebook with PySpark
```bash
# Method 1: Using environment variables (if configured)
pyspark

# Method 2: Direct Jupyter start
jupyter notebook
```

### Check Versions
```bash
# Java
java -version

# Python
python3 --version

# Spark
spark-shell --version

# PySpark
python3 -c "import pyspark; print(pyspark.__version__)"

# Jupyter
jupyter --version
```

### Switch Java Versions (if needed)
```bash
# Interactive selection
sudo update-alternatives --config java

# Set back to Java 25
sudo update-alternatives --set java /usr/lib/jvm/java-25-openjdk-amd64/bin/java
```

---

## üîß TROUBLESHOOTING CHECKLIST

### Common Issues and Solutions (Ubuntu 24.04 + Virtual Environment)

#### ‚ùå Issue: "ModuleNotFoundError: No module named 'pyspark'"
**Cause:** Virtual environment not activated or PySpark not installed in venv
**Solution:**
```bash
# Check if virtual environment is activated
echo $VIRTUAL_ENV
# If empty, activate it:
source ~/spark-project/spark-env/bin/activate

# Verify PySpark is installed in venv
pip3 list | grep pyspark

# If not installed, install it
pip3 install pyspark==4.0.1
```

#### ‚ùå Issue: Jupyter can't find PySpark
**Cause:** Jupyter not installed in the same virtual environment as PySpark
**Solution:**
```bash
# Activate virtual environment
spark-activate

# Install Jupyter in the venv
pip3 install jupyter jupyterlab

# Verify both are in the same location
which python3
which jupyter
# Both should show paths in ~/spark-project/spark-env/bin/

# Restart Jupyter
```

#### ‚ùå Issue: "error: externally-managed-environment" (if not using venv)
**Cause:** Trying to install with pip outside virtual environment
**Solution:**
```bash
# Always use virtual environment (best practice)
spark-activate
pip3 install package-name

# OR use --break-system-packages flag (not recommended)
pip3 install package-name --break-system-packages
```

#### ‚ùå Issue: "JAVA_HOME is not set"
**Solution:**
```bash
# Check if JAVA_HOME is set
echo $JAVA_HOME

# If empty, add to ~/.bashrc:
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
source ~/.bashrc

# Verify
echo $JAVA_HOME
java -version
```

#### ‚ùå Issue: "Command 'pyspark' not found"
**Solution:**
```bash
# Verify SPARK_HOME
echo $SPARK_HOME

# If empty, add to ~/.bashrc:
export SPARK_HOME=/opt/spark
export PATH=$SPARK_HOME/bin:$PATH
source ~/.bashrc

# Verify
which pyspark
```

#### ‚ùå Issue: Wrong Java version (shows Java 25 instead of 17)
**Solution:**
```bash
# Check current Java
java -version

# Switch to Java 17
sudo update-alternatives --set java /usr/lib/jvm/java-17-openjdk-amd64/bin/java
sudo update-alternatives --set javac /usr/lib/jvm/java-17-openjdk-amd64/bin/javac

# Update JAVA_HOME in ~/.bashrc
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
source ~/.bashrc

# Verify
java -version
```

#### ‚ùå Issue: Virtual environment activation doesn't work
**Solution:**
```bash
# Make sure you're using the full path
source ~/spark-project/spark-env/bin/activate

# Check if the directory exists
ls ~/spark-project/spark-env/

# If missing, recreate the virtual environment
cd ~/spark-project
python3 -m venv spark-env
```

#### ‚ùå Issue: "Permission denied" when accessing /opt/spark
**Solution:**
```bash
# Fix ownership
sudo chown -R $USER:$USER /opt/spark

# Verify
ls -ld /opt/spark
```

#### ‚ùå Issue: Jupyter doesn't open in browser
**Solution:**
```bash
# Ubuntu 24.04 may have different default browser
# Method 1: Start without browser and copy URL
jupyter lab --no-browser

# Copy the URL (http://localhost:8888/lab?token=...) and paste in browser

# Method 2: Specify browser explicitly
BROWSER=firefox jupyter lab
# or
BROWSER=google-chrome jupyter lab
```

#### ‚ùå Issue: Packages installed but Python can't find them
**Cause:** Installed packages outside virtual environment
**Solution:**
```bash
# Check where Python is looking
python3 -c "import sys; print('\n'.join(sys.path))"

# If virtual environment is activated, should see:
# /home/yourusername/spark-project/spark-env/lib/python3.12/site-packages

# If not activated:
spark-activate
python3 -c "import sys; print('\n'.join(sys.path))"
```

#### ‚ùå Issue: "bash: spark-activate: command not found"
**Cause:** Alias not defined or .bashrc not reloaded
**Solution:**
```bash
# Use full activation command
source ~/spark-project/spark-env/bin/activate

# OR add the alias to .bashrc
echo "alias spark-activate='source ~/spark-project/spark-env/bin/activate'" >> ~/.bashrc
source ~/.bashrc

# Verify
type spark-activate
```

#### ‚ùå Issue: Slow pip install on Ubuntu 24.04
**Solution:**
```bash
# Some packages build from source (normal on Ubuntu 24.04)
# Ensure build tools are installed:
sudo apt install build-essential python3-dev -y

# Then try installing again
pip3 install package-name
```

#### ‚ùå Issue: Multiple Python versions causing confusion
**Solution:**
```bash
# In virtual environment, always use python3 (not python)
which python3
# Should show: /home/yourusername/spark-project/spark-env/bin/python3

# Verify version
python3 --version
# Should show: Python 3.12.3
```

#### ‚ùå Issue: Spark UI (localhost:4040) not accessible
**Solution:**
```bash
# Make sure a Spark session is running
# Check if Spark application is active

# Verify firewall (Ubuntu 24.04)
sudo ufw status

# If active, allow localhost
sudo ufw allow from 127.0.0.1
```

#### ‚ùå Issue: "Kernel died" in Jupyter
**Cause:** Usually memory issue or Python error
**Solution:**
```bash
# Check logs in Jupyter terminal
# Check system resources
free -h

# Try reducing Spark memory:
# In notebook, before creating SparkSession:
import os
os.environ['SPARK_DRIVER_MEMORY'] = '2g'
```

### Virtual Environment Specific Tips

**Reset virtual environment if corrupted:**
```bash
# Deactivate if active
deactivate

# Remove old environment
rm -rf ~/spark-project/spark-env

# Create fresh environment
cd ~/spark-project
python3 -m venv spark-env

# Activate and reinstall from requirements.txt
source spark-env/bin/activate
pip3 install -r requirements.txt
```

**Check what's installed in virtual environment:**
```bash
# Activate environment
spark-activate

# List all packages
pip3 list

# List only project packages (not dependencies)
pip3 list --not-required

# Show package details
pip3 show pyspark
```

**Verify virtual environment is working correctly:**
```bash
# Activate environment
spark-activate

# Run verification
python3 << 'EOF'
import sys
import os

print(f"Python: {sys.executable}")
print(f"VIRTUAL_ENV: {os.environ.get('VIRTUAL_ENV', 'NOT SET')}")
print(f"In venv: {'spark-env' in sys.prefix}")

import pyspark
print(f"PySpark: {pyspark.__file__}")
EOF
```

---

## üìù INSTALLATION SUMMARY CHECKLIST

Use this final checklist to confirm everything is ready on **Ubuntu 24.04 LTS with Virtual Environment**:

### Core Components
- [ ] **Java 17 LTS** installed from Ubuntu repositories (`openjdk-17-jdk`)
- [ ] Java 17 set as default (not Java 25)
- [ ] JAVA_HOME environment variable set to `/usr/lib/jvm/java-17-openjdk-amd64`
- [ ] Can switch between Java 17 and Java 25 using `update-alternatives`

### Python & Virtual Environment
- [ ] **Python 3.12.3** working (Ubuntu 24.04 default)
- [ ] pip3 and python3-venv installed
- [ ] Virtual environment created at `~/spark-project/spark-env/`
- [ ] Can activate virtual environment with `spark-activate` or `source ~/spark-project/spark-env/bin/activate`
- [ ] Virtual environment activation changes prompt to show `(spark-env)`

### Apache Spark
- [ ] **Apache Spark 4.0.1** extracted to `/opt/spark`
- [ ] SPARK_HOME environment variable set to `/opt/spark`
- [ ] Spark binaries (`pyspark`, `spark-shell`) in PATH
- [ ] Can run `spark-shell --version` successfully (from any directory)

### PySpark & Libraries (in Virtual Environment)
- [ ] **PySpark 4.0.1** installed in virtual environment (matches Spark version)
- [ ] NumPy, Pandas, PyArrow, Matplotlib installed in venv
- [ ] PySpark can be imported when venv is activated
- [ ] Can create SparkSession successfully in venv
- [ ] `requirements.txt` file created with `pip3 freeze`

### Jupyter Notebook (in Virtual Environment)
- [ ] **Jupyter Notebook** and JupyterLab installed in venv
- [ ] Jupyter can launch when venv is activated
- [ ] Python 3.12 kernel available in Jupyter
- [ ] Can import PySpark in Jupyter notebook
- [ ] Notebooks directory created at `~/spark-project/notebooks/`
- [ ] Start script created at `~/spark-project/start-jupyter.sh`

### Validation & Testing
- [ ] Can activate venv and all packages are available
- [ ] Can create and manipulate DataFrames in Python script
- [ ] Can create and manipulate DataFrames in Jupyter
- [ ] Spark UI accessible at `http://localhost:4040` when Spark session is running
- [ ] Test notebook runs without errors

### Environment Configuration

**Verify your `~/.bashrc` contains (global settings):**
```bash
# ========== Java Configuration for Spark ==========
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH

# ========== Apache Spark Configuration ==========
export SPARK_HOME=/opt/spark
export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
export PYSPARK_PYTHON=python3

# ========== Virtual Environment Shortcut ==========
alias spark-activate='source ~/spark-project/spark-env/bin/activate'
```

**Note:** We do NOT set `PIP_BREAK_SYSTEM_PACKAGES=1` because we're using virtual environments!

### Project Structure

Verify your project structure looks like this:
```
~/spark-project/
‚îú‚îÄ‚îÄ spark-env/              # Virtual environment
‚îÇ   ‚îú‚îÄ‚îÄ bin/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ python3
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pip3
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jupyter
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ activate
‚îÇ   ‚îî‚îÄ‚îÄ lib/
‚îÇ       ‚îî‚îÄ‚îÄ python3.12/
‚îÇ           ‚îî‚îÄ‚îÄ site-packages/
‚îÇ               ‚îú‚îÄ‚îÄ pyspark/
‚îÇ               ‚îú‚îÄ‚îÄ numpy/
‚îÇ               ‚îú‚îÄ‚îÄ pandas/
‚îÇ               ‚îú‚îÄ‚îÄ jupyter/
‚îÇ               ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ notebooks/              # Your Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ spark_test_notebook.ipynb
‚îú‚îÄ‚îÄ requirements.txt        # Package list
‚îî‚îÄ‚îÄ start-jupyter.sh        # Startup script
```

### Quick Verification Commands

**From any directory (global):**
```bash
java -version                    # Should show Java 17
python3 --version               # Should show Python 3.12.3
echo $JAVA_HOME                 # Should show /usr/lib/jvm/java-17-openjdk-amd64
echo $SPARK_HOME                # Should show /opt/spark
which spark-shell               # Should find spark-shell
spark-shell --version           # Should show version 4.0.1
```

**In virtual environment:**
```bash
# Activate first
spark-activate

# These should all work:
which python3                    # Should show path in spark-env/bin/
which pip3                       # Should show path in spark-env/bin/
which jupyter                    # Should show path in spark-env/bin/
echo $VIRTUAL_ENV                # Should show ~/spark-project/spark-env
python3 -c "import pyspark; print(pyspark.__version__)"  # Should show 4.0.1
pip3 list | grep pyspark         # Should show pyspark 4.0.1
```

### Daily Workflow Verification

Test your daily workflow:
```bash
# 1. Open terminal
# 2. Activate environment
spark-activate

# Prompt should change to: (spark-env) user@hostname:~$

# 3. Navigate to project
cd ~/spark-project/notebooks

# 4. Start Jupyter
jupyter lab

# 5. Work in Jupyter (browser opens automatically)

# 6. Stop Jupyter (Ctrl+C in terminal)

# 7. Deactivate (optional)
deactivate

# Prompt returns to: user@hostname:~$
```

---

## üéì NEXT STEPS

Once all checkboxes above are complete:

1. ‚úÖ **Installation Complete on Ubuntu 24.04 with Virtual Environment!**
2. üìö **Learn Virtual Environment Best Practices**
3. üöÄ **Create Sample PySpark Projects** - Practice with DataFrames and transformations
4. üí° **Explore Spark UI** - Learn to monitor jobs at `http://localhost:4040`
5. üìä **Work with Different Data Formats** - CSV, JSON, Parquet
6. üéØ **Transition to Databricks** - Same PySpark API, managed Spark clusters

### Recommended Learning Path

**Week 1: Fundamentals**
- Virtual environment management
- Basic RDD operations
- DataFrame creation and simple queries
- Reading CSV files

**Week 2: Data Transformation**
- Spark SQL operations
- Filtering, selecting, and aggregating
- Joins and unions
- Window functions

**Week 3: Data Formats**
- Working with JSON
- Parquet file format
- Writing partitioned data
- Performance optimization

**Week 4: Advanced Topics**
- UDFs (User Defined Functions)
- Caching and persistence
- Broadcast variables
- Accumulators

**Week 5+: Production Ready**
- Error handling
- Logging and monitoring
- Performance tuning
- Move to Databricks

---

## üìö USEFUL RESOURCES FOR UBUNTU 24.04 + SPARK

### Official Documentation
- **Spark 4.0.1 Docs:** https://spark.apache.org/docs/4.0.1/
- **PySpark API Reference:** https://spark.apache.org/docs/4.0.1/api/python/
- **Spark Programming Guide:** https://spark.apache.org/docs/4.0.1/rdd-programming-guide.html
- **Spark SQL Guide:** https://spark.apache.org/docs/4.0.1/sql-programming-guide.html

### Code Examples & Tutorials
- **Spark Examples (GitHub):** https://github.com/apache/spark/tree/v4.0.1/examples/src/main/python
- **PySpark Tutorial:** https://spark.apache.org/docs/4.0.1/api/python/getting_started/index.html

### Virtual Environment Resources
- **Python venv Documentation:** https://docs.python.org/3/library/venv.html
- **PEP 668 Explanation:** https://peps.python.org/pep-0668/

### Databricks Learning (for later)
- **Databricks Learning:** https://www.databricks.com/learn
- **Databricks Community Edition:** https://community.cloud.databricks.com/ (Free tier)
- **Databricks Academy:** https://www.databricks.com/learn/training

### Ubuntu 24.04 Specific
- **Ubuntu Python Guide:** https://documentation.ubuntu.com/ubuntu-for-developers/reference/availability/python/
- **Ubuntu Package Search:** https://packages.ubuntu.com/

---

## üîÑ MAINTAINING YOUR SETUP

### Keep Your System Updated

```bash
# Update Ubuntu packages (monthly)
sudo apt update && sudo apt upgrade -y

# Update Java if needed
sudo apt install openjdk-17-jdk openjdk-17-jre
```

### Update Virtual Environment Packages

```bash
# Activate environment
spark-activate

# List outdated packages
pip3 list --outdated

# Update specific package
pip3 install --upgrade package-name

# Update all packages (careful!)
pip3 list --outdated | cut -d ' ' -f1 | xargs -n1 pip3 install -U

# Update Jupyter
pip3 install --upgrade jupyter jupyterlab

# Update requirements.txt after changes
pip3 freeze > ~/spark-project/requirements.txt
```

### Backup Your Configuration

```bash
# Backup your .bashrc
cp ~/.bashrc ~/.bashrc.backup

# Backup virtual environment packages list
pip3 freeze > ~/spark-project/requirements-backup-$(date +%Y%m%d).txt

# Backup Jupyter notebooks
cp -r ~/spark-project/notebooks ~/spark-project/notebooks-backup-$(date +%Y%m%d)
```

### Clone Environment for Different Projects

```bash
# Create a new virtual environment for another project
python3 -m venv ~/another-project/env

# Activate it
source ~/another-project/env/bin/activate

# Install from your saved requirements
pip3 install -r ~/spark-project/requirements.txt

# Or start fresh with just what you need
pip3 install pyspark==4.0.1 jupyter
```

---

## üí° VIRTUAL ENVIRONMENT TIPS

### Why Virtual Environments are Better

‚úÖ **Isolation** - Each project has its own dependencies  
‚úÖ **No system pollution** - System Python stays clean  
‚úÖ **Reproducibility** - requirements.txt recreates environment anywhere  
‚úÖ **Multiple versions** - Different projects can use different package versions  
‚úÖ **Easy cleanup** - Just delete the folder  
‚úÖ **Best practice** - Industry standard for Python development  

### Quick Reference

```bash
# Create new venv
python3 -m venv my-env

# Activate
source my-env/bin/activate

# Deactivate
deactivate

# Delete (when outside venv)
rm -rf my-env

# Save packages
pip3 freeze > requirements.txt

# Install from file
pip3 install -r requirements.txt
```

---

**Document Version:** 3.0 (Virtual Environment Optimized)  
**Last Updated:** December 26, 2024  
**Platform:** Ubuntu 24.04 LTS (Noble Numbat) + Apache Spark 4.0.1 + Python 3.12.3 + Virtual Environment  
**Approach:** Best Practice with Python Virtual Environments  
**Tested On:** Fresh Ubuntu 24.04 LTS installation
