# Databricks Certified Data Engineer Associate - Complete Study Guide

**Generated:** January 5, 2026, 04:18:57 UTC

---

## Exam Overview

- **Questions:** 45 multiple-choice questions
- **Duration:** 90 minutes
- **Passing Score:** 70%
- **Cost:** $200 USD
- **Validity:** 2 years
- **Recommended Experience:** 6+ months hands-on data engineering with Databricks

---

## Exam Domain Breakdown

### 1. Databricks Lakehouse Platform (24%)
### 2. ELT with Spark SQL and Python (29%)
### 3. Incremental Data Processing (22%)
### 4. Production Pipelines (16%)
### 5. Data Governance (9%)

---

## 1. Databricks Lakehouse Platform (24%)

### Lakehouse Architecture
- **Definition:** Combines data lake flexibility with data warehouse performance
- **Key Benefits:**
  - Single platform for BI and ML
  - ACID transactions on data lakes
  - Schema enforcement and evolution
  - Unified governance
  - Cost-effective storage

### Architecture Components

#### Control Plane vs Data Plane
- **Control Plane:** Databricks-managed services
  - Web application
  - Cluster management
  - Notebook management
  - Jobs scheduler
  - Security and access control
  
- **Data Plane:** Customer cloud account
  - Worker nodes
  - Driver nodes
  - DBFS storage
  - Customer data

### Databricks Workspace

#### Key Components
1. **Workspace Browser**
   - Organize notebooks, folders, dashboards
   - Shared vs personal workspaces
   - Navigation structure

2. **Clusters**
   - **All-Purpose Clusters:** Interactive development, shared across users
   - **Job Clusters:** Automated workloads, terminated after job completion
   - **Cluster Modes:**
     - Standard: General purpose
     - High Concurrency: Multi-user, table ACLs
     - Single Node: Development/testing
   - **Cluster Pools:** Pre-allocated VMs for faster startup
   - **Auto-scaling:** Dynamic worker allocation
   - **Auto-termination:** Cost saving feature

3. **Notebooks**
   - Multi-language support (Python, SQL, Scala, R)
   - Collaborative editing
   - Cell-level execution
   - **Magic Commands:**
     - `%sql` - SQL code
     - `%python` - Python code
     - `%scala` - Scala code
     - `%md` - Markdown documentation
     - `%sh` - Shell commands
     - `%fs` - File system commands
     - `%run` - Run another notebook

4. **Databricks Utilities (dbutils)**
   - `dbutils.fs.*` - File system operations
   - `dbutils.secrets.*` - Secret management
   - `dbutils.widgets.*` - Interactive widgets/parameters
   - `dbutils.notebook.*` - Notebook workflows

### Databricks File System (DBFS)
- Abstraction layer over cloud storage (S3, ADLS, GCS)
- Mounted as `/dbfs` in notebooks
- Standard paths:
  - `/FileStore` - User files
  - `/databricks-datasets` - Sample datasets
  - `/mnt` - Mounted storage

### Repos and Git Integration
- **Git Operations:** clone, commit, push, pull, branch
- **Advantages over Notebook Versioning:**
  - Branch management
  - Pull requests
  - Team collaboration
  - External backup
  - CI/CD integration

---

## 2. ELT with Spark SQL and Python (29%)

**CRITICAL SECTION - Largest exam portion**

### Creating Tables

#### Managed Tables
```sql
CREATE TABLE employees (
  id INT,
  name STRING,
  salary DOUBLE
)
USING DELTA
LOCATION '/path/to/data';  -- Optional
```
- Data managed by Databricks
- Dropped when table is dropped

#### External Tables
```sql
CREATE EXTERNAL TABLE employees
LOCATION '/mnt/data/employees';
```
- Data remains after table drop
- Only metadata deleted

#### CREATE TABLE AS SELECT (CTAS)
```sql
CREATE TABLE high_earners AS
SELECT * FROM employees
WHERE salary > 100000;
```

#### CREATE OR REPLACE TABLE AS SELECT (CRAS)
```sql
CREATE OR REPLACE TABLE daily_summary AS
SELECT date, COUNT(*) as count
FROM transactions
GROUP BY date;
```

### Writing Data to Tables

#### INSERT INTO
```sql
INSERT INTO employees
VALUES (1, 'John Doe', 75000);

INSERT INTO employees
SELECT * FROM new_hires;
```

#### INSERT OVERWRITE
```sql
INSERT OVERWRITE employees
SELECT * FROM updated_employees;
```
âš ï¸ **Warning:** Replaces ALL data in table

#### COPY INTO
```sql
COPY INTO employees
FROM '/mnt/raw/employees/'
FILEFORMAT = PARQUET;
```
- Idempotent operation
- Tracks processed files
- Good for incremental loads

#### MERGE (Upsert)
```sql
MERGE INTO employees target
USING updates source
ON target.id = source.id
WHEN MATCHED THEN
  UPDATE SET *
WHEN NOT MATCHED THEN
  INSERT *;
```

### Data Cleaning

#### Handling NULLs
```sql
-- Filter out nulls
SELECT * FROM table WHERE column IS NOT NULL;

-- Replace nulls
SELECT COALESCE(column, 'default_value') FROM table;

-- Fill nulls in DataFrame
df.fillna({'column1': 0, 'column2': 'unknown'})
```

#### Deduplication
```sql
-- Using DISTINCT
SELECT DISTINCT * FROM table;

-- Using ROW_NUMBER
SELECT * FROM (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY timestamp DESC) as rn
  FROM table
) WHERE rn = 1;
```

```python
# PySpark
df.dropDuplicates(['id'])
```

#### Data Validation
```sql
-- Check constraints
ALTER TABLE employees 
ADD CONSTRAINT valid_salary CHECK (salary > 0);
```

### Combining and Reshaping Data

#### JOINs
```sql
-- INNER JOIN
SELECT * FROM orders o
INNER JOIN customers c ON o.customer_id = c.id;

-- LEFT JOIN
SELECT * FROM orders o
LEFT JOIN customers c ON o.customer_id = c.id;

-- ANTI JOIN (rows in left not in right)
SELECT * FROM orders o
ANTI JOIN returns r ON o.order_id = r.order_id;

-- SEMI JOIN (rows in left that have match in right)
SELECT * FROM customers c
SEMI JOIN orders o ON c.id = o.customer_id;
```

#### UNION
```sql
-- UNION (removes duplicates)
SELECT * FROM march_transactions
UNION
SELECT * FROM april_transactions;

-- UNION ALL (keeps duplicates)
SELECT * FROM march_transactions
UNION ALL
SELECT * FROM april_transactions;
```

#### Window Functions
```sql
SELECT 
  employee_id,
  salary,
  AVG(salary) OVER (PARTITION BY department) as dept_avg,
  RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank
FROM employees;
```

#### Aggregations
```sql
-- Basic aggregation
SELECT department, AVG(salary), COUNT(*)
FROM employees
GROUP BY department;

-- ROLLUP (hierarchical subtotals)
SELECT department, job_title, SUM(salary)
FROM employees
GROUP BY department, job_title WITH ROLLUP;

-- CUBE (all combinations)
SELECT department, job_title, SUM(salary)
FROM employees
GROUP BY department, job_title WITH CUBE;
```

#### PIVOT and UNPIVOT
```sql
-- PIVOT
SELECT * FROM (
  SELECT year, quarter, revenue
  FROM sales
)
PIVOT (
  SUM(revenue)
  FOR quarter IN ('Q1', 'Q2', 'Q3', 'Q4')
);
```

### SQL User-Defined Functions (UDFs)

```sql
-- Create UDF
CREATE FUNCTION mask_ssn(ssn STRING)
RETURNS STRING
RETURN CONCAT('XXX-XX-', RIGHT(ssn, 4));

-- Use UDF
SELECT mask_ssn(ssn) FROM employees;
```

### Python and PySpark Integration

#### Converting Between DataFrames and SQL
```python
# Register DataFrame as temp view
df.createOrReplaceTempView("temp_table")

# Query using SQL
result = spark.sql("SELECT * FROM temp_table WHERE age > 30")

# Access table as DataFrame
df = spark.table("employees")
```

#### String Manipulation
```python
from pyspark.sql.functions import col, concat, upper, lower, regexp_replace

df.select(
    upper(col("name")).alias("name_upper"),
    concat(col("first_name"), col("last_name")).alias("full_name"),
    regexp_replace(col("phone"), r'\D', '').alias("phone_clean")
)
```

#### Control Flow
```python
from pyspark.sql.functions import when, col

df.withColumn(
    "salary_category",
    when(col("salary") > 100000, "High")
    .when(col("salary") > 50000, "Medium")
    .otherwise("Low")
)
```

### Complex Data Types

#### Working with Arrays
```sql
-- Array functions
SELECT 
  array_contains(skills, 'Python') as has_python,
  size(skills) as skill_count,
  explode(skills) as individual_skill
FROM employees;
```

#### Working with Structs
```sql
-- Struct access
SELECT 
  address.city,
  address.zip_code
FROM employees;
```

#### Working with JSON
```sql
-- Parse JSON string
SELECT 
  get_json_object(json_string, '$.name') as name,
  get_json_object(json_string, '$.age') as age
FROM raw_data;
```

```python
# PySpark JSON
from pyspark.sql.functions import from_json, schema_of_json

schema = spark.read.json(df.select("json_column").rdd.map(lambda x: x[0])).schema
df.withColumn("parsed", from_json(col("json_column"), schema))
```

---

## 3. Incremental Data Processing (22%)

### Structured Streaming Concepts

#### Core Principles
- Treats data as unbounded table
- Incremental processing
- Fault-tolerant, exactly-once semantics
- Event-time processing

#### Triggers
```python
# Process as data arrives (default)
.trigger(processingTime='5 seconds')

# Process all available data then stop
.trigger(once=True)

# Continuous processing (low latency)
.trigger(continuous='1 second')
```

#### Output Modes
- **Append:** Only new rows (default for most streams)
- **Complete:** Entire result table
- **Update:** Only updated rows

```python
query = (spark.readStream
    .format("delta")
    .table("source_table")
    .writeStream
    .format("delta")
    .outputMode("append")
    .option("checkpointLocation", "/checkpoints/query1")
    .table("target_table")
)
```

#### Checkpointing
- Tracks processing progress
- Enables fault tolerance
- Required for all streaming queries
- Stores offset information

```python
.option("checkpointLocation", "/mnt/checkpoints/my_stream")
```

#### Watermarking
Handles late-arriving data
```python
from pyspark.sql.functions import window

df.withWatermark("event_time", "10 minutes") \
  .groupBy(window("event_time", "5 minutes")) \
  .count()
```

### Auto Loader

#### Overview
- Incrementally ingest files from cloud storage
- Automatically detects new files
- Schema inference and evolution
- Exactly-once processing

#### File Notification Mode
```python
df = (spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "json")
    .option("cloudFiles.useNotifications", "true")  # Event-based
    .load("/mnt/raw/data/")
)
```

#### Directory Listing Mode
```python
df = (spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "csv")
    .option("cloudFiles.useNotifications", "false")  # Polling-based
    .load("/mnt/raw/data/")
)
```

#### Schema Inference and Evolution
```python
df = (spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "parquet")
    .option("cloudFiles.schemaLocation", "/schemas/my_table")
    .option("cloudFiles.schemaEvolutionMode", "addNewColumns")
    .load("/mnt/raw/data/")
)
```

### Multi-Hop Architecture (Medallion)

#### Bronze Layer (Raw)
- Raw data ingestion
- Exact copy of source
- Minimal transformation
- Append-only
```python
(spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "json")
    .load("/mnt/landing/")
    .writeStream
    .format("delta")
    .option("checkpointLocation", "/checkpoints/bronze")
    .table("bronze.raw_events")
)
```

#### Silver Layer (Cleaned)
- Cleaned and conformed
- Schema enforcement
- Deduplication
- Data quality checks
```python
(spark.readStream
    .table("bronze.raw_events")
    .filter(col("id").isNotNull())
    .dropDuplicates(["id"])
    .writeStream
    .format("delta")
    .option("checkpointLocation", "/checkpoints/silver")
    .table("silver.clean_events")
)
```

#### Gold Layer (Business-Level)
- Aggregated metrics
- Business logic applied
- Optimized for analytics
- Often materialized views
```python
(spark.readStream
    .table("silver.clean_events")
    .groupBy("customer_id", window("event_time", "1 day"))
    .agg(count("*").alias("event_count"))
    .writeStream
    .format("delta")
    .option("checkpointLocation", "/checkpoints/gold")
    .outputMode("complete")
    .table("gold.daily_customer_metrics")
)
```

### Delta Live Tables (DLT)

#### Declarative Pipeline Definition

**SQL Syntax:**
```sql
-- Streaming table
CREATE STREAMING LIVE TABLE bronze_orders
AS SELECT * FROM cloud_files('/data/orders', 'json');

-- Materialized view
CREATE LIVE TABLE silver_orders
AS SELECT * FROM LIVE.bronze_orders WHERE order_id IS NOT NULL;

-- View (not materialized)
CREATE LIVE VIEW gold_revenue
AS SELECT date, SUM(amount) as revenue
FROM LIVE.silver_orders
GROUP BY date;
```

**Python Syntax:**
```python
import dlt
from pyspark.sql.functions import col

@dlt.table
def bronze_orders():
    return (spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .load("/data/orders")
    )

@dlt.table
def silver_orders():
    return (dlt.read_stream("bronze_orders")
        .filter(col("order_id").isNotNull())
    )
```

#### Data Quality Expectations
```python
@dlt.table
@dlt.expect("valid_id", "id IS NOT NULL")
@dlt.expect_or_drop("valid_amount", "amount > 0")
@dlt.expect_or_fail("valid_date", "date IS NOT NULL")
def validated_orders():
    return dlt.read("bronze_orders")
```

- `@dlt.expect`: Record violations, continue processing
- `@dlt.expect_or_drop`: Drop invalid records
- `@dlt.expect_or_fail`: Fail pipeline on violations

#### DLT Features
- Automatic dependency management
- Data lineage tracking
- Data quality metrics dashboard
- Automatic error handling and retries
- Change Data Capture (CDC) support

---

## 4. Production Pipelines (16%)

### Databricks Workflows (Jobs)

#### Job Components
1. **Tasks:** Individual units of work
   - Notebook task
   - JAR task
   - Python script
   - DLT pipeline
   - SQL query

2. **Task Dependencies:**
   - Linear dependencies
   - Fan-out/fan-in patterns
   - Conditional execution

#### Job vs All-Purpose Clusters
| Feature | Job Cluster | All-Purpose Cluster |
|---------|-------------|---------------------|
| Lifecycle | Created per job run | Persistent |
| Cost | Lower (auto-terminates) | Higher |
| Use Case | Production jobs | Interactive development |
| Sharing | Not shared | Shared across users |

#### Creating a Job
```python
# Configure job via UI or API
{
    "name": "Daily ETL Pipeline",
    "tasks": [
        {
            "task_key": "extract",
            "notebook_task": {
                "notebook_path": "/Workflows/Extract"
            },
            "new_cluster": {...}
        },
        {
            "task_key": "transform",
            "depends_on": [{"task_key": "extract"}],
            "notebook_task": {
                "notebook_path": "/Workflows/Transform"
            }
        }
    ],
    "schedule": {
        "quartz_cron_expression": "0 0 2 * * ?",
        "timezone_id": "UTC"
    }
}
```

#### Job Scheduling
- **Cron expressions:** Flexible scheduling
- **File arrival triggers:** Event-driven
- **API triggers:** Programmatic execution

#### Retry Logic
```python
{
    "max_retries": 3,
    "retry_on_timeout": true,
    "timeout_seconds": 3600
}
```

#### Task Parameters
```python
# In notebook
dbutils.widgets.get("date")

# In job definition
{
    "base_parameters": {
        "date": "2024-01-01"
    }
}
```

#### Monitoring Jobs
- Job runs history
- Task execution times
- Cluster metrics
- Error logs
- Email/webhook alerts

### Databricks SQL

#### SQL Warehouses (Compute)
- **Serverless:** Managed by Databricks, instant startup
- **Classic:** Customer-managed VMs
- **Pro:** Enhanced performance, photon engine
- **Serverless Pro:** Best of both

#### Warehouse Configuration
- Size (X-Small to 4X-Large)
- Auto-stop after inactivity
- Scaling (min/max clusters)
- Spot instance usage

#### Queries
- Save and share queries
- Query history
- Query parameters
- Scheduled query execution

#### Dashboards
- Visualizations (charts, maps, counters)
- Multiple queries per dashboard
- Auto-refresh
- Dashboard filters
- Share with stakeholders

#### Alerts
- Query-based conditions
- Email/webhook notifications
- Custom schedules
- Alert history

### Cost Optimization

#### Strategies
1. **Cluster Configuration:**
   - Use job clusters for production
   - Enable auto-termination
   - Right-size clusters
   - Use cluster pools

2. **Compute Options:**
   - Serverless for on-demand workloads
   - Spot instances for fault-tolerant jobs
   - Auto-scaling for variable loads

3. **Storage:**
   - VACUUM old Delta versions
   - OPTIMIZE to reduce file count
   - Partition pruning

4. **SQL Warehouses:**
   - Auto-stop settings
   - Appropriate sizing
   - Serverless for intermittent use

---

## 5. Data Governance (9%)

### Unity Catalog

#### Three-Level Namespace
```
catalog.schema.table
```
- **Catalog:** Top-level container (e.g., `production`, `dev`)
- **Schema:** Database/namespace (e.g., `sales`, `marketing`)
- **Table:** Table or view (e.g., `customers`, `orders`)

#### Metastore
- Central repository for metadata
- One metastore per region
- Attached to workspaces
- Stores catalog, schema, table definitions

#### Key Features
- Unified governance across clouds
- Centralized permissions
- Data lineage
- Audit logging
- Search and discovery

### Entity Permissions

#### Permission Types
- `SELECT`: Read data
- `MODIFY`: Insert, update, delete
- `CREATE`: Create objects
- `USAGE`: Access namespace
- `ALL PRIVILEGES`: All permissions

#### GRANT Syntax
```sql
-- Grant table access
GRANT SELECT ON TABLE catalog.schema.table TO user@example.com;

-- Grant schema access
GRANT USAGE ON SCHEMA catalog.schema TO `analysts`;

-- Grant catalog access
GRANT USAGE ON CATALOG catalog TO `data_team`;

-- Grant create permissions
GRANT CREATE TABLE ON SCHEMA catalog.schema TO `engineers`;
```

#### REVOKE Syntax
```sql
REVOKE SELECT ON TABLE catalog.schema.table FROM user@example.com;
```

### Table ACLs (Access Control Lists)

#### Enabling Table ACLs
- Requires High Concurrency cluster
- Configured in cluster settings

```sql
-- Show grants
SHOW GRANTS ON TABLE catalog.schema.table;
```

### Row-Level and Column-Level Security

#### Dynamic Views
```sql
CREATE VIEW sales_filtered AS
SELECT 
  order_id,
  customer_id,
  amount,
  CASE 
    WHEN is_member('sales_team') THEN customer_name
    ELSE 'REDACTED'
  END as customer_name
FROM sales
WHERE 
  CASE 
    WHEN is_member('managers') THEN true
    WHEN is_member('sales_reps') THEN sales_rep = current_user()
    ELSE false
  END;
```

#### Column Masking
```sql
CREATE VIEW employees_masked AS
SELECT 
  employee_id,
  name,
  CASE 
    WHEN is_member('hr') THEN ssn
    ELSE 'XXX-XX-XXXX'
  END as ssn
FROM employees;
```

### Data Lineage
- Automatically tracks data flow
- Column-level lineage
- Visual representation in UI
- Helps with impact analysis

### Tagging and Comments

#### Marking PII Data
```sql
CREATE TABLE customers (
  id INT,
  name STRING,
  ssn STRING COMMENT 'PII: Social Security Number'
)
COMMENT 'Contains customer PII data'
TBLPROPERTIES ('contains_pii' = 'true');
```

#### Viewing Metadata
```sql
DESCRIBE TABLE EXTENDED catalog.schema.table;
```

---

## Delta Lake Deep Dive

### Core Features
- ACID transactions
- Time travel
- Schema enforcement and evolution
- Audit history
- Upserts and deletes

### Essential Commands

#### OPTIMIZE
```sql
-- Compact small files
OPTIMIZE table_name;

-- With Z-ordering
OPTIMIZE table_name ZORDER BY (column1, column2);
```
- Reduces number of files
- Improves query performance
- Runs in background

#### Z-ORDERING
- Colocates related data
- Improves filter performance
- Specify up to 4 columns
```sql
OPTIMIZE events ZORDER BY (date, user_id);
```

#### VACUUM
```sql
-- Remove files older than retention period (default 7 days)
VACUUM table_name;

-- Custom retention
VACUUM table_name RETAIN 168 HOURS;
```
âš ï¸ **Warning:** Breaks time travel to vacuumed versions

#### Time Travel
```sql
-- Query historical version
SELECT * FROM table_name VERSION AS OF 5;

-- Query at timestamp
SELECT * FROM table_name TIMESTAMP AS OF '2024-01-01';

-- Restore previous version
RESTORE TABLE table_name TO VERSION AS OF 10;
```

#### DESCRIBE HISTORY
```sql
DESCRIBE HISTORY table_name;
```
- Shows version history
- Operation types
- Timestamps
- User information

#### CLONE
```sql
-- Deep clone (copies data)
CREATE TABLE table_copy DEEP CLONE source_table;

-- Shallow clone (references data)
CREATE TABLE table_ref SHALLOW CLONE source_table;
```

#### MERGE
```sql
MERGE INTO target t
USING source s
ON t.id = s.id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *;
```

#### DELETE
```sql
DELETE FROM table_name WHERE condition;
```

#### UPDATE
```sql
UPDATE table_name 
SET column = value
WHERE condition;
```

### Schema Management

#### Schema Enforcement
```sql
-- Attempt to insert incompatible data fails
INSERT INTO employees VALUES ('invalid_id', 'John');  -- Error!
```

#### Schema Evolution
```sql
-- Allow schema evolution
SET spark.databricks.delta.schema.autoMerge.enabled = true;

-- Merge schema on write
df.write.format("delta")
  .option("mergeSchema", "true")
  .mode("append")
  .save("/path/to/table")
```

### Table Constraints

```sql
-- Add constraint
ALTER TABLE employees 
ADD CONSTRAINT valid_age CHECK (age >= 18 AND age <= 120);

-- Drop constraint
ALTER TABLE employees 
DROP CONSTRAINT valid_age;
```

### Generated Columns
```sql
CREATE TABLE events (
  event_time TIMESTAMP,
  event_date DATE GENERATED ALWAYS AS (CAST(event_time AS DATE))
);
```

### Change Data Feed (CDF)
```sql
-- Enable CDF
ALTER TABLE table_name
SET TBLPROPERTIES (delta.enableChangeDataFeed = true);

-- Read changes
SELECT * FROM table_changes('table_name', 0, 10);
```

---

## Apache Spark Fundamentals

### Architecture

#### Distributed Computing Model
- **Driver:** Orchestrates execution, maintains SparkContext
- **Executors:** Perform actual computation
- **Cluster Manager:** Resource allocation (YARN, Mesos, Kubernetes, Standalone)

#### Lazy Evaluation
- Transformations build execution plan
- Actions trigger computation
- Optimizations applied before execution

### Transformations vs Actions

#### Transformations (Lazy)
- `select()`, `filter()`, `groupBy()`, `join()`
- `map()`, `flatMap()`, `distinct()`
- Return new DataFrame/RDD

#### Actions (Eager)
- `count()`, `collect()`, `show()`
- `write()`, `save()`, `take()`
- Trigger computation

### DataFrames vs RDDs

#### DataFrames (Preferred)
- Structured API
- Catalyst optimizer
- Tungsten execution engine
- Better performance

#### RDDs (Lower-level)
- Functional programming API
- More control
- Legacy code

### Partitioning

```python
# Repartition (shuffle)
df.repartition(10)

# Coalesce (reduce without shuffle)
df.coalesce(5)

# Partition by column
df.repartition("date")
```

### Broadcast Joins
```python
from pyspark.sql.functions import broadcast

large_df.join(broadcast(small_df), "key")
```

---

## Key Study Tips

### 1. Hands-On Practice (Critical!)
- Create free Databricks Community Edition account
- Practice ALL SQL commands
- Build streaming pipelines
- Configure jobs and workflows
- Work with Unity Catalog

### 2. Focus Areas by Weight
**High Priority:**
- ELT operations (29%)
- Lakehouse Platform (24%)
- Incremental Processing (22%)

**Medium Priority:**
- Production Pipelines (16%)
- Data Governance (9%)

### 3. Common Exam Traps
- VACUUM breaks time travel
- OPTIMIZE vs Z-ORDER differences
- Job cluster vs all-purpose cluster
- MERGE syntax and conditions
- Streaming trigger types
- Checkpoint locations required for streams
- COPY INTO is idempotent
- INSERT OVERWRITE replaces all data

### 4. Must-Know Commands
```sql
-- Delta Lake
OPTIMIZE, VACUUM, MERGE, DESCRIBE HISTORY, RESTORE

-- Table Operations
CREATE TABLE, CTAS, CRAS, INSERT, COPY INTO

-- Governance
GRANT, REVOKE, SHOW GRANTS

-- Schema
DESCRIBE TABLE EXTENDED
```

### 5. Recommended Resources
- Databricks Academy free courses
- Official exam guide
- Databricks documentation
- Practice exams (Udemy, SkillCertPro)
- O'Reilly book: "Databricks Certified Data Engineer Associate Study Guide"

### 6. Practice Questions Focus
- Table creation methods
- Stream processing concepts
- Job configuration
- Delta Lake operations
- Unity Catalog permissions
- Multi-hop architecture
- Data quality with DLT

---

## Quick Reference Commands

### Delta Lake
```sql
OPTIMIZE table ZORDER BY (col1, col2)
VACUUM table RETAIN 168 HOURS
DESCRIBE HISTORY table
RESTORE TABLE table TO VERSION AS OF 5
```

### Table Operations
```sql
CREATE TABLE name AS SELECT...
COPY INTO table FROM 'path' FILEFORMAT = format
MERGE INTO target USING source ON condition
INSERT OVERWRITE table SELECT...
```

### Streaming
```python
.trigger(once=True)
.trigger(processingTime='5 seconds')
.option("checkpointLocation", "/path")
.outputMode("append")
```

### Unity Catalog
```sql
GRANT SELECT ON TABLE catalog.schema.table TO user
REVOKE SELECT ON TABLE catalog.schema.table FROM user
SHOW GRANTS ON TABLE catalog.schema.table
```

---

## Final Exam Tips

1. **Read questions carefully** - Watch for keywords like "always," "never," "best"
2. **Eliminate wrong answers** - Use process of elimination
3. **Time management** - 2 minutes per question average
4. **Flag and return** - Don't get stuck on hard questions
5. **Hands-on experience beats memorization** - Actually build pipelines
6. **Understand WHY, not just HOW** - Know when to use each feature
7. **Practice with realistic questions** - Use practice exams
8. **Review wrong answers** - Learn from mistakes

---

## Good Luck! ðŸŽ¯

Remember: This certification validates practical skills. Focus on hands-on practice over memorization!
